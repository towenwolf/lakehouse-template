# Apache Spark Configuration Template for Lakehouse
# Customize these settings based on your cluster size and workload

# Application Properties
spark.app.name=lakehouse-data-product
spark.master=yarn  # or spark://host:port for standalone, local[*] for local mode

# Spark SQL and DataFrame Settings
spark.sql.warehouse.dir=/user/hive/warehouse
spark.sql.catalogImplementation=hive  # or 'in-memory' for testing
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true

# Delta Lake / Iceberg Configuration (choose one)
# For Delta Lake:
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# For Apache Iceberg (alternative):
# spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
# spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
# spark.sql.catalog.spark_catalog.type=hive

# Memory Settings
spark.driver.memory=4g
spark.executor.memory=8g
spark.executor.memoryOverhead=1g

# Core and Parallelism
spark.executor.cores=4
spark.default.parallelism=200
spark.sql.shuffle.partitions=200

# Dynamic Allocation
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.minExecutors=2
spark.dynamicAllocation.maxExecutors=20
spark.dynamicAllocation.initialExecutors=4

# Shuffle Settings
spark.shuffle.service.enabled=true
spark.shuffle.compress=true
spark.io.compression.codec=snappy

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false

# Logging
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs:///spark-logs
spark.history.fs.logDirectory=hdfs:///spark-logs

# Metrics
spark.metrics.conf.*.sink.prometheus.class=org.apache.spark.metrics.sink.PrometheusServlet
spark.metrics.conf.*.sink.prometheus.path=/metrics/prometheus
spark.ui.prometheus.enabled=true

# S3/Cloud Storage Settings (AWS example)
# spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain
# spark.hadoop.fs.s3a.endpoint=s3.amazonaws.com
# spark.hadoop.fs.s3a.fast.upload=true

# Azure Storage (alternative)
# spark.hadoop.fs.azure.account.key.youraccount.dfs.core.windows.net=YOUR_KEY

# GCS Storage (alternative)
# spark.hadoop.google.cloud.auth.service.account.enable=true
# spark.hadoop.google.cloud.auth.service.account.json.keyfile=/path/to/keyfile.json
